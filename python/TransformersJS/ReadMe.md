[LLM-ul de tip Transformers](https://stirileprotv.ro/stiri/i-like-ai/ce-sunt-si-cum-functioneaza-modele-lingvistice-mari-llm-afla-cum-poti-sa-le-folosesti.html) in Javascript

Am citit acest [post-HF](https://huggingface.co/posts/Xenova/648607043613090) postat de cei de la [***Xenova***](https://github.com/topics/xenova-transformers?l=javascript) si am gasit proiectul [***WebGPU***](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU)(un [succesor/inlocuitor](https://huggingface.co/blog/transformersjs-v3#webgpu-support) al renderului de grafica 2D/3D [WebGL](https://en.wikipedia.org/wiki/WebGL))

**Remarca**: numele celor de la [**Xenova**](https://www.npmjs.com/package/@xenova/transformers) si al celor de [**HuggingFace(HF)**](https://huggingface.co/docs/transformers.js/api/models) este strans legate de [**Transformers.js**](https://www.npmjs.com/package/@xenova/transformers), care la randul [acestuia](https://huggingface.co/blog/luigi12345/transformers-js), se bazeaza pe/[utilizeaza](https://onnxruntime.ai/getting-started) [**ONNX runtime**](https://onnxruntime.ai/). pentru a putea rula local, [modele](https://huggingface.co/docs/transformers.js/custom_usage#convert-your-models-to-onnx), direct in **browser**.

***Nota***: [Hugging Face(HF)](https://medium.com/@kenzic/run-models-in-the-browser-with-transformers-js-2d0983ba3ce9) este un fel de depozitar(un fel de CDN, asa cum este si Github, dar...) pt [ML](https://medium.com/@fareedkhandev/transformers-js-ai-in-the-browser-zero-server-costs-maximum-privacy-2cd8d28798b7)

Iata ce se spune(traducere cu mici ["intariri/comentarii"](https://huggingface.co/docs/hub/transformers-js) prin anumite/cateva locuri:) in aceasta [postare](https://huggingface.co/docs/transformers/index):

Tocmai s-a lansat [**Transformers.js v3.1**](https://github.com/huggingface/transformers.js/releases/tag/3.0.0)( a se vedea [***exemplele-demo***](https://github.com/huggingface/transformers.js-examples)) de la [**HF**](https://websim.ai/p/tzubbb91ul3tgj_fhzfz/13) È™i nu o sÄƒ vÄƒ vinÄƒ sÄƒ credeÈ›i, ce este posibil(in **jasvascript/js**, se cam poate face, ce era posibil pana acum, de facut, *doar* in **python/py**), acum, Ã®n [**browser**](https://medium.com/@kenzic/run-models-in-the-browser-with-transformers-js-2d0983ba3ce9) cu [acceleratorul](https://github.com/microsoft/Phi-3CookBook/blob/main/md/08.Update/Phi35/031.WebGPUWithPhi35Readme.md) de [redare grafica](https://developer.chrome.com/docs/web-platform/webgpu/troubleshooting-tips) [***WebGPU***](https://huggingface.co/docs/transformers.js/guides/webgpu)! <br/><br/>ğŸ¤¯ SÄƒ aruncÄƒm o [privire](https://community.neptune-software.com/topics/neptune-dxp/blogs/running--transformers-in-the--browser-with--neptune--s) asupra modelelor(uneori modelele pot fi folosite in cascada,....adica/i.e unul dupa altul,... <br/>...in functie de scopul urmarit, se foloseste un model sau altul):<br/>
 - ğŸ”€ [**Janus**](https://huggingface.co/deepseek-ai/Janus-1.3B) de la [***Deepseek***](https://github.com/dzhng/deep-seek) - pentru Ã®nÈ›elegerea È™i generarea multi**modalÄƒ** unificatÄƒ (Text-to(**2**)-Image È™i Image-Text-**2**-Text)<br/>
 - ğŸ‘ï¸ [**Qwen2-VL**](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d) de la [***Qwen***](https://github.com/QwenLM/Qwen)  - pentru Ã®nÈ›elegerea imaginii cu rezoluÈ›ie dinamicÄƒ(Vision-Language=**VL**)<br/>
 - ğŸ”¢ [**JinaCLIP**](https://huggingface.co/jinaai/jina-clip-v1) de la [***Jina AI***](https://github.com/jina-ai) - pentru general -Ã®nglobare multi**modalÄƒ** multi**lingvÄƒ** cu scop<br/>(*cross-modal similarity*...se compara **texte** obtinute/[extrase](https://github.com/Programming-from-A-to-Z/transformers-js-examples) prin mai multe **modal**itati.. texte,imagini-text....avem conducta [**ETL**](https://en.wikipedia.org/wiki/Extract,_transform,_load) ar fi urmatoarea: dupa **E**xtractie, textele sunt eventual transcrise intr-o limba anume sau daca vreti sunt **T**ransformate si apoi inglobate/incorporate/memorarate/incarcate**L**oad in VectorDB:, iar apoi, se compara/incruciseaza/**cross** un **Text**-pur/brut/raw cu un alt text, extras dintr-o imagine/**Imagine-Text**, in vederea stabilirii **similaritatii**)<br/>
 - ğŸŒ‹ [**LLaVA-OneVision**](https://huggingface.co/docs/transformers/main/model_doc/llava_onevision) de la [***ByteDance***](https://github.com/bytedance) - pentru generarea Image-Text-**2**-Text<br/>
 - ğŸ¤¸â€â™€ï¸ [**ViTPose**](https://huggingface.co/qubvel-hf/vitpose-base) - pentru estimarea poziÈ›iei corporale(Vision Transformer=**ViT**)<br/>
 - ğŸ“„ [**MGP-STR**](https://huggingface.co/docs/transformers/model_doc/mgp-str) - pentru recunoaÈ™terea opticÄƒ a caracterelor (**OCR**)<br/>
 - ğŸ“ˆ [**PatchTST**](https://huggingface.co/docs/transformers/model_doc/patchtst) È™i [**PatchTSMixer**](https://huggingface.co/docs/transformers/model_doc/patchtsmixer) - pentru prognoza serii cronologice<br/><br/>

AÈ™a este, totul se ruleazÄƒ [local](https://github.com/huggingface/transformers.js/tree/main), [100%](https://microsoft.github.io/onnxruntime-web-demo/#/) Ã®n  [**browser**](https://www.youtube.com/watch?v=yGVte5KubRQ) (niu sunt transmise date la/catre un server)! ğŸ”¥ UriaÈ™ pentru confidenÈ›ialitate!<br/><br/>

ConsultaÈ›i [notele de lansare](https://github.com/huggingface/transformers.js/releases/tag/3.0.0) pentru mai multe [informaÈ›ii](https://huggingface.co/docs/transformers.js/index). ğŸ‘‡<br/>
https://github.com/huggingface/transformers.js/releases/tag/3.1.0<br/>


Link [demonstrativ](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU) (+ [cod sursÄƒ](https://github.com/huggingface/transformers.js-examples/tree/main/janus-webgpu)):
[webml-community/Janus-1.3B-WebGPU](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU)

<br/><br/>In proiectul [***Janus-1.3B-WebGPU***](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU) am gasit urmatoarea descriere/prezentare:

SunteÈ›i pe cale sÄƒ Ã®ncÄƒrcaÈ›i [Janus-1.3B](https://huggingface.co/onnx-community/Janus-1.3B-ONNX), un [model](https://www.tensorflow.org/js/models)(**LLM**) multimodal de **viziune** - limbaj care este optimizat pentru inferenÈ›Äƒ pe web. 

Totul ruleazÄƒ 100% local Ã®n [browser](https://github.com/whitphx/transformers.js.py)ul tÄƒu cu ğŸ¤— [Transformers.js](https://huggingface.co/docs/transformers.js) È™i [ONNX Runtime Web](https://onnxruntime.ai/docs/tutorials/web/)(permite sÄƒ rulaÈ›i È™i sÄƒ implementaÈ›i [modele](https://www.tensorflow.org/js/models) de ML Ã®n aplicaÈ›ia dvs. web, folosind JS), ceea ce Ã®nseamnÄƒ cÄƒ <u>nu sunt trimise date</u> cÄƒtre un server. 

OdatÄƒ ce [modelul](https://towardsdatascience.com/nlp-transformers-pipelines-with-onnx-9b890d015723) s-a Ã®ncÄƒrcat, acesta poate fi folosit chiar È™i [offline](https://github.com/m1ckc3b/object-detection-with-transformerjs). 

[Codul sursÄƒ](https://github.com/huggingface/transformers.js-examples/tree/main/janus-webgpu) pentru [demo](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU) poate fi gÄƒsit pe [GitHub](https://github.com/huggingface/transformers.js-examples/tree/main/janus-webgpu).

<hr/>

 Un alt [post/articol](https://do-me.github.io/SemanticFinder/) interesant este acesta legat de [**SemanticSearch**](https://www.reddit.com/r/ollama/comments/1b79c23/inbrowser_rag_feeding_ollama/).

Iata ce se spune in acest [articol](https://do-me.github.io/SemanticFinder/):

 **RAG Ã®n browser care hrÄƒneÈ™te/alimenteaza**(cu date contextuale salvate/recuperate [d]in memoria/BD-virtuala) **Ollama**<br/>
<br/>BunÄƒ, oameni buni,<br/><br/>
M-am gÃ¢ndit sÄƒ Ã®mpÄƒrtÄƒÈ™esc integrarea **Ollama** Ã®n [**SemanticFinder**](https://github.com/do-me/SemanticFinder) , un [instrument](https://www.reddit.com/r/ollama/comments/1b79c23/inbrowser_rag_feeding_ollama/?tl=it) de [cÄƒutare semanticÄƒ](https://do-me.github.io/SemanticFinder/) Ã®n **browser**(pe orice sau <br/>indiferent de [*dispozitiv*](https://github.com/modzy/hugging-face-raspberry-pi)).<br/>
[**Transformers.js**](https://github.com/huggingface/transformers.js) se ocupÄƒ de tokenizarea intrÄƒrii È™i de rularea modelului.<br/>
S-a lucrat intens la instrumente de cÄƒutare semanticÄƒ la nivelul clientului Ã®n ultimul an, pentru a le permite atÃ¢t persoanelor neprofesioniste, cÃ¢t È™i experÈ›ilor sÄƒ foloseascÄƒ cele mai recente modele de Ã®ncorporare fÄƒrÄƒ a fi nevoie sÄƒ instaleze nimic.

Folosind [**Transformers.js**](https://github.com/huggingface/transformers.js), este destul de nebunesc cÄƒ (aproape) toate modelele/**LLM**(sub cÃ¢È›iva GB) pot rula Ã®n Ã®ntregime Ã®n [**browser**](https://github.com/praeclarum/transformers-js)<br/> folosind [**ONNX**](https://www.reddit.com/r/MachineLearning/comments/rr17f9/p_45_times_faster_hugging_face_transformer/)!<br/>
La fel procedaÈ›i È™i cu **LLM**-urile mici, cum ar fi cele din familia [**Qwen**](https://huggingface.co/Qwen) sau [**LaMini-Flan-T5**](https://huggingface.co/MBZUAI/LaMini-Flan-T5-783M).<br/>
Se descurcÄƒ grozav pentru dimensiunea lor, dar dacÄƒ doriÈ›i o calitate È™i mai bunÄƒ, trebuie sÄƒ rulaÈ›i un server Ã®n afara **browser**ului dvs. deoarece: dimensiunea fiÈ™ierului/utilizarea RAM este limitatÄƒ È™i, prin urmare, dimensiunea È™i contextul modelului sunt reduse.<br/>
Sunt sigur cÄƒ aceasta este o limitare care va dispÄƒrea Ã®n cele din urmÄƒ, dar pentru moment este necesar sÄƒ rulaÈ›i un server de inferenÈ›Äƒ(cum este si **Ollama**) Ã®n afara **browser**ului dumneavoastrÄƒ.<br/>

Aici intervine **Ollama**: completeazÄƒ [**SemanticFinder**](https://do-me.github.io/SemanticFinder/) È™i sunt rezultatele cÄƒutÄƒrii semantice pentru a pune Ã®ntrebÄƒri despre ele.<br/>

De exemplu, puteÈ›i cÄƒuta Ã®n Ã®ntreaga [biblie](https://github.com/do-me/SemanticFinder?tab=readme-ov-file#catalogue)(a se vedea sub-tag-ul *Catalogue*) Ã®n **browser** <br/>â€mÃ¢ncÄƒm mÃ¢ncare gustoasÄƒâ€<br/> È™i dupÄƒ ce cereÈ›i unui **LLM** sÄƒ obtinem un rezumat, de exemplu, de forma <br/>â€Pe baza urmÄƒtorului context, rÄƒspundeÈ›i la Ã®ntrebarea: Ce fel de mÃ¢ncare mÄƒnÃ¢ncÄƒ? <br/>Context: **SEARCH_RESULTS** ".

Recuperarea Ã®n **RAG** este un fel de manualÄƒ Ã®n acest moment, aÈ™a cÄƒ sunteÈ›i cam "spoonfeed"(hranire cu lingura-deci manuala nu automata) **LLM**, dar vreau sÄƒ adaug un automatism pentru aceasta (de exemplu, Ã®ntrebÃ¢nd **LLM**-ul Ã®nsuÈ™i cum ar arÄƒta o interogare **RAG** idealÄƒ, rulÃ¢nd-o È™i hrÄƒnind aceasta cu rezultatele).

Deoarece toate componentele sunt modificabile, ele funcÈ›ioneazÄƒ frumos Ã®n multe limbi sau chiar Ã®nglobÄƒri de cod (de exemplu, folosind Ã®nglobarea codului [**Jina**](https://huggingface.co/jinaai/jina-embeddings-v2-base-code)).

Spune-mi ce crezi! :)

DacÄƒ doriÈ›i sÄƒ-l testaÈ›i singur, asiguraÈ›i-vÄƒ cÄƒ permiteÈ›i [**CORS**](https://do-me.github.io/SemanticFinder/) setÃ¢nd variabila **env** Ã®n consecinÈ›Äƒ:

- pe *Windows* , de sub *Powershell*: $env:**OLLAMA_ORIGINS**="[https://do-me.github.io](https://github.com/do-me/trending-huggingface-models/)"; **ollama serve**
- pe *Ubuntu*: **OLLAMA_ORIGINS**="[https://do-me.github.io](https://github.com/do-me/)" **ollama serve**

<hr/>

Am ales, aici, in acest articol, sa implementez doar un simplu [exemplu](https://huggingface.co/docs/transformers.js/tutorials/vanilla-js) de etichetare a obiectelor dintr-o [imagine](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/road.png) data([**Computer-Vision(CV)**](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index1.js)).

Exemplul nostru, incarca [**Transformers.js**](https://cdn.jsdelivr.net/npm/@xenova/transformers@2.8.0), din *internet*(dintr-un [CDN](https://en.wikipedia.org/wiki/Content_delivery_network)(special pregatit/destinat pt livrarea de continut: module/pachete/librarii), si, din aceasta cauza, **nu** este o varianta 100% *off-line*(pt ca are nevoie de *internet*, resursele aflandu-se la distanta ... caz in care, in anumite circumstante, trebuie sa acordati atentie si [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing)-ului), dar... se poate cu usurinta, face,  ca [acesta](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index1.js), sa ruleze *offline*(adica, descarcam resursele din internet, intr-un director *local*, si apoi, in [script]((https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index1.js)), le folosim pe acestea, in locul celor aflate la distanta/in internet).

In schimb, dupa incarcarea resurselor din internet, [totul](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index1.js) se  ruleaza *offline**, in browser(deci *local*)

De asemena folosesc un [webserver](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/1.webserver.bat) adhoc si build-in oferit de python pt a expune local fisierul [**index.html**](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index.html)

Pe parte de [client](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/2.chrome_client), am deschis pagina [**index.html**](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index.html), in browser-ul **Chrome**. Pagina "sigleton/[SPA](https://libres.uncg.edu/ir/asu/f/Sapphire_Willow_Spring%202022_Thesis.pdf)" **html** este simplu stilizata cu ajutorul fisierului [**style.css**](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/style.css)

Partea de cod, din acest script [index1.js](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/index1.js), care prezinta un deosebit interes/pe care va rog sa va concentrati/focusati, este selectia *speciala* de linii de cod: ...

     import { pipeline } from "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.8.0";
     ...
     const detector = await pipeline("object-detection", "Xenova/yolos-tiny");
     ...
     const detectedObjet = await detector(image.src, {
                                                        threshold: 0.5,
                                                        percentage: true,
                                                      });


... in rest, regasim un cod-javascript, cat se poate de *obisnuit/comun*!

In prima linie, se incarca din internet, din [CDN](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing)-ul celor de la ***jsdelivr.net***, mai precis din sectiunea **npm**(daca vreti, sectiunea special destinata incarcatorului ***NodeJS***, mai precis/adica ***npm.exe***, deci accesibila si surselor **JS**-cum este si cazul de aici), se incarca deci din spatiul alocat celor de la **xenova**, modulul [**transformers**](https://github.com/huggingface/transformers.js/blob/main/README.md), versionat ca fiind **2.8.0**, in acest proiect, dar, care in acest moment, a ajuns la versionea **3.1.1**)

Apoi folosim o linie/conducta de procesare(*pipeline*) locala, pentru a construi un **detector de obiecte**, obiecte care sunt prezente/memorate/aflate, intr-un obiect-imagine(cum este *image.src*); Pentru detectie folosim modelul **yolos-tiny**, oferit de catre cei de la ***xenova***. Acest model de dimensiune redusa este special destinat detectiei de obiecte(dintr-o imagine data), detectie local-procesata(pe parte de client) cu resurse minime, intr-un browser(adecvat versionat); Este un model cross-browser(pt ca este acceptat de multe browsere renumite, printre care si Chrome);

Si in fine, in ultima linie de cod, regasim pur si simplu, *utilizarea/folosirea/invocarea* acestui **detector** si nimic mai mult(aplicata obiectului [***image.src***](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/road.png) si utilizand o anumita *lista de parametrii*, specifici acestui detector/***{...}***, de forma *parametru:valoare*)!

Modul de rulare este unul simplu(asa cum v-ati/am obisnuit cred!!!:), mai intai( rulati [**web*sever***](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/1.webserver.bat)-ul ***1.webserver.bat***(nu-l inchideti ci doar il minimizati) si apoi rulati [**client**ul](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/TransformersJS/2.chrome_client) **2.chrome_client.bat**.

Acest detector de ***Computer-Vision(CV)***, va *identifica* obiectele din imaginea local-incarcata, si, in cele din urma, le va *eticheta*, in consecinta/ca atare.

Modelul este unul [*pre-antrenat*](https://medium.com/@kenzic/run-models-in-the-browser-with-transformers-js-2d0983ba3ce9), astfel ca, va "sti", cum sa "procedeze", atunci cand, va fi utilizat in [aplicatiile](https://dev.to/kenzic/run-models-in-the-browser-with-transformersjs-7g8) dvs [practice](https://blog.logrocket.com/building-background-remover-vue-transformers-js/).
