Nomenclatorul **UAT**-urilor, cunoscut sub denumirea [**SIRUTA**](https://siruta.nxm.ro/) este intretinut si versionat la 6 luni de catre [**INS**](https://www.cpsa.nxm.ro/) dar poate fi descarcat ca dataset(uneori gasiti versiuni mai vechi ale acestuia deoarece propagarea dureaza!!!:) si de pe site-ul [***UE***](https://data.europa.eu/data/datasets/9f38f6fe-66a0-4e93-ae24-4272b91c9849?locale=ro); 

Am gasit si pe ***github*** acest nomenclator (My)SQL [**SIRUTA**](https://github.com/bandizsolt/romanian-counties-and-locations) si chiar si o [***biblioteca***](https://github.com/strainu/SIRUTA) scrisa in **python**.

Iata biblioteca Python pentru analizarea unui extras [**SIRUTA**](https://github.com/strainu/SIRUTA) Ã®n format [CSV](https://github.com/mgax/workshop-odss-vis/blob/master/rawdata/siruta-judete.csv) sau [json](https://github.com/mgax/harta-cim/blob/gh-pages/siruta.json).

Desigur puteti cauta acest nomenclator **SIRUTA** si pe [data.gov.ro](https://data.gov.ro/dataset?res_format=csv)

[**SIRUTA**](https://github.com/GeorgianStan/romania-uat-api?tab=readme-ov-file) poate fi folosit si pentru alte scopuri(altele decat cele legate de ANAF/MFP/GOV) cum ar fi [alegeri](https://github.com/alexaac/presidentials2019Data), [ANRE](https://github.com/posfgit/standard), [GIS](https://github.com/akaleeroy/romania-uat) / [GeoSpatial](https://github.com/geospatialorg/scripturi-actualizare-vectori/tree/master)...

[**SIRUTA**](https://ro.wikipedia.org/wiki/SIRUTA) are o ***granulatie*** sau o ***rezolutie*** pana la nivel de [***localitate***](https://github.com/andreifurrnica/localitati-romania)

[INS](https://bucuresti.insse.ro/produse-si-servicii/nomenclatoare-statistice/) se mai ocupa si de alte nomenclatore cum ar fi 
[**CAEN**, **CPSA**](https://www.anaf.ro/anaf/internet/ANAF/asistenta_contribuabili/declararea_obligatiilor_fiscale/coduri_caen)

Puteti afla mai multe depre aceste 2 nomenclatoare CAEN si CPSA folosind aceasta [cautare simpla google](https://www.google.com/search?q=caen+cpsa&rlz=1C1JJTC_enRO1087RO1087&oq=CAEN&gs_lcrp=EgZjaHJvbWUqCAgAEEUYJxg7MggIABBFGCcYOzIGCAEQRRg5Mg8IAhAuGEMYsQMYgAQYigUyBwgDEAAYgAQyBwgEEAAYgAQyBwgFEAAYgAQyBwgGEAAYgAQyBwgHEAAYgAQyBwgIEAAYgAQyBwgJEAAYgATSAQkzMTkxajBqMTWoAgiwAgE&sourceid=chrome&ie=UTF-8#ip=1)

Pentru cei care doresc sa caute date pe data.gov.ro(ckan) pot studia mai intai aceasta [pagina](https://data.gov.ro/pages/developers) dedicata comunitatii dezvoltatorilor

Exista si un [downloader](https://github.com/deathy/data-gov-ro-data-download) pt [data.gov.ro/CKAN](https://data.gov.ro/en/dataset?q=&tags=lista&res_format=xlsx&sort=metadata_modified+desc), script scris in python( si acest [repository](https://github.com/sanand0/data.gov.in) chiar daca este pentru India ci nu pt Romania, poate fi desigur un model pt voi in incercarea de a descarca datele de care aveti nevoie).

Iata aici am gasit un depozit care [listeaza primariile](https://github.com/vimishor/dataset-primarii) din Romania.

Puteti accesa acest [depozit](https://github.com/petre-renware/api_to_roefact) ce prezinta un sistem de facturare scris in pyton folosind excel,
pentru a va completa informatiile cu privire la sistemul eFactura.

<hr/>

Daca sunteti interesati sa folositi [***"AI/NLP generative"***](https://community.sap.com/t5/technology-blogs-by-members/harnessing-langchain-for-rag-enhanced-private-gpt-development-on-sap-btp/ba-p/13576303) pt **facturi**(si in general pt ingerarea oricarui [tip/fel de document/informatie](https://community.sap.com/t5/technology-blogs-by-members/creating-an-advanced-private-gpt-leveraging-rag-concepts-and-langchain-for/ba-p/13576515), indiferend de nivelul sau de [structurare](https://unstructured.io/) si de organizare: ierarhic/relational=logice/graf=retea de date/informatii/cunostinte)  puteti gasi un [exemplu](https://github.com/ronidas39/LLMtutorial/tree/main/tutorial91) bazat pe [langchain](https://www.langchain.com/)(un cadru AI-integrator extrem de popular).

Cititi in prealabil documentul powerpoint [tutorial91.pptx](https://github.com/ronidas39/LLMtutorial/blob/main/tutorial91/tutorial91.pptx) explicativ.

Si in general puteti posta pe google[/imagini] "intrebari(a.k.a interogari)" gen ["python langchain invoice generator UBL2.1"](https://www.google.com/search?q=python+langchain+invoice+generator+UBL2.1&sca_esv=2a19a3414e05e997&rlz=1C1JJTC_enRO1087RO1087&udm=2&biw=1536&bih=762&sxsrf=ADLYWIJX2J6d2m2qJ7RgnKQp92zQTSSk7w%3A1721659370891&ei=6m-eZuWCNv-L7NYPj9SamAk&ved=0ahUKEwjln-f58LqHAxX_BdsEHQ-qBpMQ4dUDCBA&uact=5&oq=python+langchain+invoice+generator+UBL2.1&gs_lp=Egxnd3Mtd2l6LXNlcnAiKXB5dGhvbiBsYW5nY2hhaW4gaW52b2ljZSBnZW5lcmF0b3IgVUJMMi4xSL0eUKcHWPUbcAF4AJABAJgBdqABygKqAQMwLjO4AQPIAQD4AQGYAgCgAgCYAwDiAwUSATEgQIgGAZIHAKAHhwE&sclient=gws-wiz-serp)
pentru a gasi si alte [oportunitati](https://medium.com/@muskankhandelwal369/querying-the-accounting-standards-using-langchain-78b9942302c7) asemanatoare.

O alta IA/[NLP](https://github.com/bhavyabhagerathi/Invoice-Data-Extraction-Bot-using-LLAMA-2-and-Streamlit)-aplicatie intereseanta(ChatGPT-ul vostru/nostru in materie de facturi PDF) ar putea fi  sa zicem, una avand urmatorul scenariu:

-am un director plin de facturi generate [PDF](https://ijrpr.com/uploads/V5ISSUE4/IJRPR25137.pdf)(obtinute chiar/poate din imaginile xml/UBL2.1)  si...

-as dori sa intreb(utilizand sa zicem vocea... necesita ASR pt voice2text) :  Ce total are factura cu numarul 999999?, iar...

-raspunsul ar putea sa sune(necesita conversie TTS text2voice) gen : 12.29 RON

restul tine doar de ...[imaginatia voastra creativa](https://www.analyticsvidhya.com/blog/2023/10/building-invoice-extraction-bot-using-langchain-and-llm/)...(in acest demers/aventura [langchain](https://www.youtube.com/watch?v=hFqIa-mX4GM&ab_channel=SharathRaju) va poate fi de ajutor:); Rezumatele unor texte mari/largi sau daca vreti [rezumarea(eng.summary)](https://thenewstack.io/how-to-summarize-large-documents-with-langchain-and-openai/) prezinta un alt tip interesant de NLP-aplicatii generative;

***OBS:*** 

daca nu doriti sa va incurcati cu Intrebare-Raspuns(QA) de tip vocal/sunet atunci puteti renunta la acest mod/tip de interactiune cu utilizatorul si puteti lucra direct pe text ca orice alt chat(conventional) dar...
pastrand totusi  specificitatea 
 - datelor tale/proprii(cele preluate din directorul sursa ce contine fisierele PDF)  precum si ... a
 - limbajului romanesc utilizat atat la text de interogare cat si ca text de redare/valorificare/recuperare/raportare(Q&A in limba RO)

[Langchain](https://python.langchain.com/v0.2/docs/concepts/) este o librarie [integratoare](https://nanonets.com/blog/langchain/) de alte instrumente(prin inlantuire).
Acest cadru-integrator(framework) are un foarte bun suport pt. o multime de alte produse-software(SW) cum ar fi : 
- [LLM](https://mindsdb.com/blog/navigating-the-llm-landscape-a-comparative-analysis-of-leading-large-language-models)-uri(creierul oricarui sistem Q&A: ex. [RAG](https://python.langchain.com/v0.2/assets/images/rag_landscape-627f1d0fd46b92bc2db0af8f99ec3724.png)) de tot [felul](https://gathnex.medium.com/breaking-chains-coheres-free-llm-api-s-shakes-openai-s-foundation-b91f6156c89d) (cel mai cunoscut fiind desigur [OpenAI](https://github.com/tiksharsh/langchain-llama2-openAI-project/tree/main) cu binecunoscutele sale modele GPT)
- ca cititor(reader/ingestor) de documente PDF la fel se poate lucra cu tot felul de librarii(ex. PyPDF dar si altele asemanatoare)
- ca si GUI puteti folosi tot asa o multime de instrumente cum ar fi [streamlit](https://docs.streamlit.io/)(local gazduit cu access prin browser http://localhost:8501), tkinter,...
- pe parte de bd relationale puteti folosi conectori si manipulatori specifici(ex. [SQLalchemy](https://github.com/langchain-ai/langchain/discussions/22340) dar nu numai!)...

```
    print('https://github.com/langchain-ai/langchain/discussions/22340')
    print('2.Setup your Mysql conector')
    from sqlalchemy import create_engine
    from langchain_community.utilities import SQLDatabase
    schema_protos="mysql+pymysql"
    username="root"
    password=""
    host="localhost" #"127.0.0.1"
    port="3306"
    database_name="students"
    DATABASE_URI = schema_protos+"://"+username+":"+password+"@"+host+":"+port+"/"+database_name
    print(DATABASE_URI);
    
    engine = create_engine(DATABASE_URI)
    sql_database = SQLDatabase(engine=engine)
    #sql_database = SQLDatabase.from_uri(DATABASE_URI)
    
    print(sql_database)
    print(sql_database.dialect)
    print(sql_database.get_usable_table_names())
    print(sql_database.run("SELECT * FROM parents LIMIT 10;"))
    print(sql_database.run("SELECT * FROM students LIMIT 10;"))
    
    print('3.Initialize a table')
    from sqlalchemy import Table, Column, Integer, String, MetaData
    
    metadata = MetaData()
    message_store = Table(
        'message_store', metadata,
        Column('id', Integer, primary_key=True),
        Column('page_content', String(255)),
        Column('metadata', String(255))
    )
    metadata.create_all(engine)
    
    print('4.Save docs')
    from langchain_core.documents import Document
    
    test_docs = [
        Document(
            page_content="Apple Granny Smith 150 0.99 1",
            metadata={"fruit_id": 1},
        ),
        Document(
            page_content="Banana Cavendish 200 0.59 0",
            metadata={"fruit_id": 2},
        ),
        Document(
            page_content="Orange Navel 80 1.29 1",
            metadata={"fruit_id": 3},
        ),
    ]
    
    with engine.connect() as connection:
        for doc in test_docs:
            connection.execute(
                message_store.insert().values(
                    page_content=doc.page_content,
                    metadata=str(doc.metadata)
                )
            )
    
    print(sql_database.get_usable_table_names())
    print(sql_database.run("SELECT * FROM message_store LIMIT 10;"))
```

- pentru intrarile de tip ***CSV*** se poate folosi spre exemplu sa zicem [pandas](https://www.analyticsvidhya.com/blog/2023/10/building-invoice-extraction-bot-using-langchain-and-llm/);
  pt intrarile de tip ***xslx/xls***(excel) exista de asemenea [agenti](https://github.com/langchain-ai/langchain/discussions/9847) specifici...
- se integreaza(direct sau indirect)cu tot felul de alte instrumente cum ar fi
  - docx2txt
  - python-dotenv
  - pdfplumber
  - pt partea de ASR si TTS puteti utiliza spre ex. [pyTTSx3](https://medium.com/@meirgotroot/building-an-advanced-voice-assistant-with-langchain-421bcead2cbb)
  -  ...
- pentru partea de redare se poate folosi [reportlab](https://pythongeeks.org/invoice-generator-with-python/)

Daca va temeti ca datele/informatiile voastre sunt prea ["sensibile/critice/intime"](https://www.reddit.com/r/LangChain/comments/1ayipqy/without_open_ai_or_gemini_api_key/) si nu doriti sa le expuneti LLM-urilor(care oricum are un anumit [cost](https://cursdeguvernare.ro/dictionar-economic/cost)) si daca detineti suficienta putere de calcul, poate ca ar fi util sa utilizati LLM-urile care ruleaza local(nu la distanta si care nu necesita internet, acestea functionand offline) de tip OpenSource(si de preferat [gratuite](https://www.kdnuggets.com/2023/05/8-free-ai-llms-playgrounds.html), acestea avand codul expus in CDN-uri gen github), situatie in care api-key/cheia-aplicatiei nu ar mai trebui sa fie necesara!

In acest ultim caz de utilizare, poate fi util sa luati in considerare si produsul [LM Studio](https://lmstudio.ai/)

Legat de gratuitate,  iata un [tutorial](https://www.youtube.com/watch?v=g0eaSMgvDnc&ab_channel=CodePort) care mi-a retinut [atentia ](https://github.com/Dipeshpal/langchain_tutorial)(atentie insa ... va trebuie resurse considerabile de computing!!!)

O [alternativa/rival](https://www.datacamp.com/blog/langchain-vs-llamaindex) la [LangChain](https://github.com/gkamradt/langchain-tutorials/tree/main) o constituie [LlaMA-index](https://docs.llamaindex.ai/en/stable/api_reference/packs/raft_dataset/). 

Mai Trebuie spus insa si faptul ca aceste [produse-SW](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb) nu sunt singurele si acestea nu se exclud ci mai degraba trebuiesc alese in functie de necesitatile voastre de [proiectare](https://medium.com/@glenn_53777/framework-to-design-an-ai-finance-accounting-assistant-a1f4b62dcde).


Oricum [AI/ML/NLP-generative](https://www.koyeb.com/tutorials/using-langserve-to-build-rest-apis-for-langchain-applications)(a se vedea subiectul **RAG**) a provocat daca vreti o adevarata [AI](https://www.hpcdan.org/reeds_ruminations/information_technology/)-[revolutie](https://icml.cc/virtual/2023/events/poster) in mai toate [domeniile](https://www.blueprism.com/resources/blog/generative-ai-equity-research-langchain/) fie ele economice sau sociale.

<hr/>

Sa ne concetram atentia asupra unei parti importante a unui RAG si anume partea de incorporare(partea de <b>transformare</b> a propozitiilor unui text in vectori numerici)
In acest sens, Nomic a publicat pe Hub-ul modelellor LLM(HuggingFace), un nou [produs](https://huggingface.co/nomic-ai/modernbert-embed-base) intitulat [**ModernBERT Embed Base**](https://www.youtube.com/watch?v=HcVav0IqZlk).

Dupa cum puteti vedea, acest nou **incorporator**, beneficiaza de caracteristici specifice importante.

**ModernBERT Embed Base** este un model de Ã®ncorporare antrenat de **ModernBERT base** , care aduce noile progrese ale ModernBERT la Ã®ncorporare!

Antrenat pe seturile de date slab supravegheate Èi supravegheate, **ModernBERT Embed Base** , acceptÄ Èi dimensiunile de 256 ***Matryoshka Representation Learning(trunchierea Matryoshka***, reducÃ¢nd memoria de 3x cu pierderi minime de performanÈÄ.

Acest model(folosit pt sarcina de incorporare) are avantajul ca poate fi folosit ***direct*** din/cu bibliotecile **Transformers(python,js)**

Sa ne ocupam putin de cazul python:

 - instalarea bibiliotecii **Transformers** utilizand instalatorul(specific python) ***pip***:
 
       pip install git+https://github.com/huggingface/transformers.git
 
 - iata un [exemplu](https://huggingface.co/nomic-ai/modernbert-embed-base) simplu de utilizare pt sarcina de **Transformare de propoziÈii/SentenceTransformer** de dimensiune mare(768):

       from sentence_transformers import SentenceTransformer
    
       model = SentenceTransformer("nomic-ai/modernbert-embed-base")
    
       query_embeddings = model.encode([
               "search_query: What is TSNE?",
               "search_query: Who is Laurens van der Maaten?",
           ])
       doc_embeddings = model.encode([
       "search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten",
       ])
          
       print(query_embeddings.shape, doc_embeddings.shape)
       # (2, 768) (1, 768)
    
       similarities = model.similarity(query_embeddings, doc_embeddings)
       print(similarities)
       # tensor([[0.7214],
       #         [0.3260]])

***OBS:*** Dupa ce aceasta sarcina de inglobare/incapsulare a fost efectuata(in acest caz sarcina s-a efectuat cu ciopartire/trunchiere de 768), s-a efectuat valorificrea
acesteia cautand similaritatile.

 - aceiasi varianta de **SentenceTransformer**, dar de data aceasta, pt o dimensiune mai mica, de 256(utilizand pt aceasta ciopartire, ***trunchierea Matryoshka***), ar arata astfel::

       from sentence_transformers import SentenceTransformer
       
       model = SentenceTransformer("nomic-ai/modernbert-embed-base", truncate_dim=256)
       
       query_embeddings = model.encode([
           "search_query: What is TSNE?",
           "search_query: Who is Laurens van der Maaten?",
       ])
       doc_embeddings = model.encode([
           "search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten",
       ])
          
       print(query_embeddings.shape, doc_embeddings.shape)
       # (2, 256) (1, 256)
       
       similarities = model.similarity(query_embeddings, doc_embeddings)
       print(similarities)
       # tensor([[0.7759],
       #         [0.3419]])

 - similar, in varianta **Transformers**, pt inceput, fara trunchiere(valorificarea mentinandu-se aceiaisi), vom avea:

       import torch
       import torch.nn.functional as F
       from transformers import AutoTokenizer, AutoModel
       
       
       def mean_pooling(model_output, attention_mask):
           token_embeddings = model_output[0]
           input_mask_expanded = (
               attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
           )
           return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
               input_mask_expanded.sum(1), min=1e-9
           )
       
       
       query_embeddings = model.encode([
               "search_query: What is TSNE?",
               "search_query: Who is Laurens van der Maaten?",
           ])
       doc_embeddings = model.encode([
       "search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten",
       ])
       
       tokenizer = AutoTokenizer.from_pretrained("nomic-ai/modernbert-embed-base")
       model = AutoModel.from_pretrained("nomic-ai/modernbert-embed-base")
       
       encoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors="pt")
       encoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors="pt")
       
       with torch.no_grad():
           queries_outputs = model(**encoded_queries)
           documents_outputs = model(**encoded_documents)
       
       query_embeddings = mean_pooling(queries_outputs, encoded_queries["attention_mask"])
       query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
       doc_embeddings = mean_pooling(documents_outputs, encoded_documents["attention_mask"])
       doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)
       print(query_embeddings.shape, doc_embeddings.shape)
       # torch.Size([2, 768]) torch.Size([1, 768])
       
       similarities = query_embeddings @ doc_embeddings.T
       print(similarities)
       # tensor([[0.7214],
       #         [0.3260]])



 - utilizarea aferenta **Transformers**, dar de aceasta data, utilizind / impreuna cu ***trunchierea Matryoshka***, de dimensiune  mica(256):

       import torch
       import torch.nn.functional as F
       from transformers import AutoTokenizer, AutoModel
       
       
       def mean_pooling(model_output, attention_mask):
           token_embeddings = model_output[0]
           input_mask_expanded = (
               attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
           )
           return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
               input_mask_expanded.sum(1), min=1e-9
           )
       
       
       query_embeddings = model.encode([
               "search_query: What is TSNE?",
               "search_query: Who is Laurens van der Maaten?",
           ])
       doc_embeddings = model.encode([
       "search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten",
       ])
       
       tokenizer = AutoTokenizer.from_pretrained(".")
       model = AutoModel.from_pretrained(".")
       truncate_dim = 256
       
       encoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors="pt")
       encoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors="pt")
              
       with torch.no_grad():
           queries_outputs = model(**encoded_queries)
           documents_outputs = model(**encoded_documents)
       
       query_embeddings = mean_pooling(queries_outputs, encoded_queries["attention_mask"])
       query_embeddings = query_embeddings[:, :truncate_dim]
       query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
       doc_embeddings = mean_pooling(documents_outputs, encoded_documents["attention_mask"])
       doc_embeddings = doc_embeddings[:, :truncate_dim]
       doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)
       print(query_embeddings.shape, doc_embeddings.shape)
       # torch.Size([2, 256]) torch.Size([1, 256])

       similarities = query_embeddings @ doc_embeddings.T
       print(similarities)
       # tensor([[0.7759],
       #         [0.3419]])

       

       
***OBS:*** Ãn cazul **Transformers**, puteÈi trunchia Ã®nglobÄrile la o dimensiune mai micÄ prin/folosind tÄierea Ã®nglobÄrilor medii grupate, Ã®nainte de normalizare.
