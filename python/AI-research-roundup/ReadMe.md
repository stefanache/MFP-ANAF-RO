Salutare,

In acest [studiu](https://www.linkedin.com/pulse/ai-research-roundup-safety-scaling-multimodal-breakthroughs-dynuf/) din seria "Rezumatelor din sfera cercetărilor AI", au aparut, o seama de termeni/prescurtări, des utiliza[ți/te].

Pentru a fi inteles, cu usurinta, acest articol, vom trece in revista, cativa dintre acesti termeni:

 - LLM - Modele de Limbaj mari(scalate la diverse marimi 1B, 3B, 8B...).
 - SFT = Supervised Fine-Tuning = reglare-Fină Supravegheată.
 - CFT = Critique Fine-Tuning = reglare Fină Critică.
 - RL  = Reinforcement Learning = învățăre prin consolidare.
 - OOD = Out-Of-Distribution = în afara Distribuției.
 - RDS = Reasoning Data Synthesis = Sinteza Datelor de Raționament
   <br/>RDS este un mod/o varianta de abordăre privind instruirea/invatare/antrenarea pt indeplinirea sarcinilor-AI.   
 - GR = [GuardReasoner](https://arxiv.org/html/2501.18492v1) = model de Gardă bazat pe Raționament(metodologia GuardReasoner).
 - R-SFT = Reasoning Supervised Fine-Tuning = reglaj-Fin Supravegheat bazat pe/de Raționament(mod/model de gândire-asemănătoare omului);
   <br/>R-SFT este un mod/o varianta de abordăre privind instruirea/invatare/antrenarea pt indeplinirea sarcinilor-AI in conformitate cu  metodologia GuardReasoner(GR).
 - DPO = Direct Preference Optimization = Optimizarea Directă a Preferințelor(pt mostre).   
 - HS-DPO = Hard-Sample Direct Preference Optimization = - Optimizarea Directă a Preferințelor pentru esantioanele/[mostrele](https://www.google.com/search?q=mostre+sau+monstre&rlz=1C1CHBF_enRO1132RO1132&oq=mostre+sau+monstre&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIKCAEQABgKGBYYHjIKCAIQABiABBiiBNIBCTYwMzhqMGoxNagCCLACAQ&sourceid=chrome&ie=UTF-8) dure(mostre/exemple/esantioane destinate invatarii/antrenamentului, si care... se află în apropierea graniței/limita de decizie/i);
   <br/>HS-DPO este un mod/o varianta de abordăre privind instruirea/invatare/antrenarea pt indeplinirea sarcinilor-AI in conformitate cu metodologia GuardReasoner(GR).

