Va propun sa urmariti [seria](https://www.youtube.com/@fahdmirza/videos) de [episoade](https://github.com/fahdmirza) Youtube([**YT**](https://www.linkedin.com/in/fahdmirza/?originalSubdomain=au)) elaborate de [**Fahd Mirza**](https://www.youtube.com/results?search_query=Fahd+Mirza+AI)([blog](https://www.fahdmirza.com/))

Mi-a atras(in mod [deosebit](https://huggingface.co/fahdmirzac)( atentia acest proiect [Web LLM Assistant](https://www.youtube.com/watch?v=txflvGG_hIc); 

In fisierul batch [**_1.RUN_install**](https://github.com/stefanache/MFP-ANAF-RO/blob/main/python/Fahd_Mirza_A%20I/_1.RUN_install.bat) veti gasi comentate(instructiunile prefixate cu ***REM***) instructiunile pentru instalare(care sunt necesare doar o singura data la momentul instalrii) cat si instructiunele active pt rulare;(*Nota*: in procesul de instalare se va crea un director al aplicatiei denumit ***Web-LLM-Assistant-Llamacpp-Ollama***); Aceasta aplicatie-exemplu permite interogarea/cautarea web, gen ***Google***, prefixand(cu semnul sau simbolul escape **/**)..., lucru care va semnala faptul ca urmeaza o cerere de cautare web, ...cererea  de interogare tastata de catre utilizator, la **prompt**-ul modelului conversational/de chat-ing(**gemma2:9b-instruct-q5_K_M**), model oferit de server-ul local(preinstalat) **Ollama**; Dupa decarcare(utilizand **git**), in interiorul directorului aplicatiei(***Web-LLM-Assistant-Llamacpp-Ollama***) ,veti gasi un fisier denumit **requirements.txt**(care contine ***dependenta*** de librariile necesare rularii: [**llama-cpp-python**](https://python.langchain.com/docs/integrations/llms/llamacpp/), [*duckduck*](https://medium.com/@garysvenson09/how-to-use-duckduckgo-search-with-python-in-langchain-157a816fa8d5), [*go_search*](https://github.com/tanghaibao/goatools/blob/main/goatools/go_search.py), [*colorama*](https://pypi.org/project/colorama/), [*requests*](https://www.w3schools.com/python/module_requests.asp), [*beautifulsoup4*](https://pypi.org/project/beautifulsoup4/) sau prescurtat [*BS4*](https://pypi.org/project/beautifulsoup4/), [*trafilatura*](https://pypi.org/project/trafilatura/), [*readchar*](https://pypi.org/project/readchar/));

Iata 2 exemple de cereri ce pot fi plasate(pe rand/succesiv) in prompter-ul cererii aplicatiei:

      /latest news on AI advancements
     
      /github ANAF

Cea de-a 2 a cerere, spre exemplu, va lansa o cerere, catre motorul de cautare, pt string-ul **github ANAF** si va reda rezultatele aferente descoperite.
 
Fisierul principal necesar rularii aplicatiei este **Web-LLM.py**.

Eu am avut urmatorul context-software(**SW**) de instalare si rulare:

- **Windows 10 Pro**
- **git**(utilitarul preinstatlat si utilizat pt descarcarea/download-area depozitului github al aplicatiei-exemplu)
- **python 3.10.0** (cu environment/mediu vrtual specific: ***venv***, pt instalarea dependentelor: librariilor de care depinde aplicatia motorului de cautare web)

Ca si context hardware(**HW**), pe PC-ul meu(**i7**, **32GB GDDR4 RAM** si **1TB SSD**) am avut la dispozitie un GPU modest(**Geforce RTX 4060**)

Pentru cei care doresc sa genereze cod(**code generative-AI**), folosind modele conversationale(**LLM**), va recomand, din aceiasi serie **YT**, aplicatia-exemplu: [***Qwen2.5 Coder 32B Instruct***](https://www.youtube.com/watch?v=tMd0FcPSei4)
 sau  [***Qwen 2.5 Coder 32B***](https://www.youtube.com/watch?v=KYvVl0UT1Sk&ab_channel=PromptEngineering)

 <hr/>
 
[Orizontul](https://www.marktechpost.com/2024/11/14/microsoft-released-llm2clip-a-new-ai-technique-in-which-a-llm-acts-as-a-teacher-for-clips-visual-encoder/) [cunoasterii](https://www.microsoft.com/en-us/research/project/llm2clip/) in domeniul [generative-AI](https://www.youtube.com/watch?v=mJ-wt79u0Ls), poate fi [largit](https://github.com/microsoft/LLM2CLIP) si prin studierea unor [lucrari](https://microsoft.github.io/LLM2CLIP/) precum cea intitulata  [LLM2CLIP](https://microsoft.github.io/LLM2CLIP/).

<hr/>

In domeniul ***generative-AI(NLP,NLU)***, mai precis in domeniul [**generarii/intelegerii-3D**](https://github.com/nv-tlabs/LLaMA-Mesh), a aparut pe Youtube(YT) un filmulet al unui [3D-Chatbot](https://www.youtube.com/watch?v=c-LbbZEkcBo) interesant, intitulat "**Nvidia [Llama-Mesh](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh) - Generate [3D](https://www.youtube.com/watch?v=PSU4wB8WkzA) [Mesh](https://huggingface.co/bartowski/LLaMA-Mesh-GGUF) and Shapes with Text - Install Locally**", cu privire la generarea in spatiu-tridimensional(3D) de retele/invelitori/suprafete([**3D-mesh/shape**](https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh)), generandu-se daca doriti, inclusiv,  coordonatele varfurilor sau nodurilor retelei [3D](https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580)(date ca/spre exemplu), pt corpuri/obiecte/solide, obtinindu-se astfel modele unificate de suprafete 3D([geometrii-triangulate](https://ro.wikipedia.org/wiki/Triangularea_unei_suprafe%C8%9Be)), pornind de la interogari textuale livrate unui model-LLM specializat(in acest caz fiind [**Zhengyi/LLaMA-Mesh**](https://github.com/nv-tlabs/LLaMa-Mesh)). 
In acest [proiect](https://github.com/nv-tlabs/LLaMA-Mesh), pentru afisarea/construirea interfetei-web-utilizator(intrarea proiectului) s-a apelat la libraria/pachetul python [**gradio**.](https://www.gradio.app/). Pentru redarea/randarea 3D(iesirea grafica a proiectului **LLaMA-Mesh**) s-a folosit libraria [**trimesh**](https://trimesh.org/). Pe partea de procesare(coloana vertebrala a proiectului) s-a utilizat pachetul [**transformers**](https://huggingface.co/docs/transformers/index). 

<hr/>

Un alt proiect legat de subtitrarea sau transcrierea/descrierea unei imagini este proiectul local(windows) care utilizeaza ca LLM, modelul [Llama3.2-vision](https://www.youtube.com/watch?v=qc99ShiPAY8)(atentie la [licenta](https://www.reddit.com/r/ollama/comments/1gxwd1j/llama_32_vision_in_the_eu/?%24deep_link=true&correlation_id=a31d354b-4ba5-42d7-a450-375f98de2cb4&post_fullname=t3_1gxwd1j&post_index=0&ref=email_digest&ref_campaign=email_digest&ref_source=email&utm_content=post_title&%243p=e_as&_branch_match_id=1376944233253091740&utm_medium=Email%20Amazon%20SES&_branch_referrer=H4sIAAAAAAAAA22P207DMBBEvyZ9Sy%2B2QwGpQgjEb6yceJss%2BKa1Q%2FrEt7NpgSckWxqd8cyup1pzedztGJ2jurU5bz3Fj53OT40yOp8QbNmITEwjRethZn%2Ba1lSjnxv1JmdZlu1PfkhBAMtN3ttgRQgKGGsReRgvizu8i7qaoBV8UqEUgSLUCQHntVFLaaeMQ8ywLtPo18ozNupuSMzobb1GnHCrD053pm9Nb7vWKHdsren2rT5254d7h2rojeRyKhXOs%2FfRBlzrNPztcjMpOryIsxfAeBaFwZIHRyOWeoMw2JAtjfF%2Ft6SZB%2Fz1BM41wJBild8LvY6pVD1uvuQ5MlMcoee0FOTTy8Qp4DfMPtIgjgEAAA%3D%3D)) deservit de serverul dvs. **Ollama**. Desigur acesta se poate utiliza si pt filmulete/video-uri avand in vedere faptul ca un astfel de obiect este format dintr-o multime sau un intreg sir/serie de imagini captate succesiv in timp.

Tot in zona [media](https://www.youtube.com/@fahdmirza) puteti consulta si acest proiect [**OmniAudio-AI model 2.6B**](https://www.youtube.com/watch?v=CrTGp60KIOA) ori proiectul [**Voice-Isolator**](https://www.youtube.com/watch?v=giblKlWtjGY).

<hr/>

Daca doriti sa testati un [**RAG multimodal(imagini si text)**](https://www.linkedin.com/in/fahdmirza/recent-activity/all/) atunci puteti consulta periodic activitatea pe [***Linkedin***](https://www.llamaindex.ai/blog/multimodal-rag-in-llamacloud) a d-lui [**Fahd Mirza**](https://www.youtube.com/watch?v=96p0-3dZTOs)(trebuie sa aveti cont pe [**LLamaCloud**](https://cloud.llamaindex.ai/login):)

<hr/>

Foarte interesant este si acest [model ASR | S2T](https://github.com/usefulsensors/moonshine?tab=readme-ov-file)(model de transcriere: Voice|Speech2Txt), rulabil local(nu necesita Ollama) pe CPU(nu necesita GPU) denumit [**Moonshine**](https://www.youtube.com/watch?v=xciiB9vmImY&t=101s).<br/>
Daca intentionati sa instalati pachetul python ***useful-moonshine-onnx***(ca mai apoi sa puteti sa rulati modelul **Moonshine** din python) pe [CPU-uri/dispozitive mici/portabile](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2b-install-the-useful-moonshine-onnx-package-to-use-moonshine-with-onnx)(de ex pe [**SBC**-ul **Raspberry Pi**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file); SBC=este un mic computer pe o singura placa) atunci va trebui sa utilizati **ONXX-Runtime**.<br/> Daca insa, utilizati un [CPU normal/obisnuit](https://github.com/usefulsensors/moonshine?tab=readme-ov-file)(laptop ori desktop-PC), atunci va trebui sa instalati pachetul python ***useful-moonshine***(ca mai apoi sa puteti sa rulati modelul **Moonshine** din python), scris in(deci bazat pe sau dependent de) frontend-ul [**Keras**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax), care la randul sau, poate avea ca backend, una dintre cele 3 optiuni posibile: [**Torch**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax), [**TensorFlow**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax), ori [**JAX**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax)(in cazul in care, ca backend, utilizati **JAX**-ul, aveti posibilitatea de a utiliza **GPU** via **CUDA**, in loc de **CPU**).<br/> 
Indiferent ca instalati si utilizati pachetul ***useful-moonshine-onnx*** ori pachetul ***useful-moonshine***, mai intai de toate, va trebui/puteti sa utilizati un manager/gestionar de environment/mediu, cum este [**uv**](https://docs.astral.sh/uv/pip/environments/), pentru a crea si activa mediul specific ce va gazdui acest proiect.<br/>
Dupa activarea mediului de lucru, veti descarca din/[clona](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax) depozitul-Github aferent, cu ajutorul utilitarului [**uv**](https://docs.astral.sh/uv/pip/environments/), dupa caz, pachetul ***useful-moonshine-onnx*** ori pachetul ***useful-moonshine***.<br/>
In cazul utilizarii pachetului ***useful-moonshine***, va mai trebui desigur, sa specificati in plus, in variabila **KERAS_BACKEND**, care backend ati ales sa fie utilizat, dintre [**Torch**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax), [**TensorFlow**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax), ori [**JAX**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#2a-install-the-useful-moonshine-package-to-use-moonshine-with-torch-tensorflow-or-jax).<br/> 
Modelul **Moonshine** are o varianta de baza(***Base**), de aproximativ ***400 MB***, in timp ce varianta sa subtire(**Tiny**) are/este in jur de **190 MB**.<br/> 
Pentru a vă face o idee despre beneficii: **Moonshine** procesează segmentele(*chunks*) audio de 10 secunde de **5 ori mai rapid** decât rivalul sau ***Whisper***, menținând aceeași [*WER*](https://www.alibabacloud.com/blog/an-overview-of-methods-to-effectively-improve-rag-performance_601725) =i.e.= [*Word-Error Rates*](https://www.galileo.ai/hallucinationindex) =i.e.= [*rata de a transcrie gresit cuvintele*](https://medium.com/the-ai-forum/implementing-advanced-rag-in-langchain-using-raptor-258a51c503c6) (sau avand chiar una mai buna!).<br/> 
Codul  de inferenta(scris in **python**), care este necesar transcrierii fisierului audio **beckett.wav**(plasat in directorul ***ASSETS_DIR***) si care utilizeaza varianta **Tiny** a modelului pre-antrenat **Moonshine**, este unul cat se poate de simplu:

             >>> import moonshine # or import moonshine_onnx
             >>> moonshine.transcribe(moonshine.ASSETS_DIR / 'beckett.wav', 'moonshine/tiny') # or moonshine_onnx.transcribe(...)

Codul va produce urmatorul text-rezultat:

             ['Ever tried ever failed, no matter try again, fail again, fail better.']

Desigur acesta este un exemplu de inceput(daca vreti ... un exemplu de testare a respectivului pachet) dar dumneavoastra puteti experimenta si alte [exemple](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#examples)(un [exemplu](https://github.com/usefulsensors/moonshine/tree/main/demo) in acest sens, este si cel al utilizarii modelului **Moonshine** din/impreuna cu binecunoscuta librarie, a celor de la ***HuggingFace***, [**Transformers**](https://github.com/usefulsensors/moonshine?tab=readme-ov-file#huggingface-transformers)).<br/> 

<hr/>

Daca aveti niste/un documente/continut(nestructurat[e]) dar care contin[e] informatii/date(structurate; structura este cunoscuta/data) atunci probabil ca ati dori sa [extrageti](https://medium.com/mindsdb/transforming-unstructured-data-into-structured-using-ai-e989c56b1442)(folosind un model/[platforma](https://mindsdb.com/) AI: spre exemplu [**MindsDB**](https://github.com/mindsdb/mindsdb?tab=readme-ov-file)) si sa [stocati](https://www.youtube.com/watch?v=zDa3qvpD5lw) aceste [date structurate](https://www.youtube.com/watch?v=zDa3qvpD5lw) intr-o baza de date(desigur [structurata](https://www.youtube.com/watch?v=dadY-cUpUm0)); Probabil ca deja ati identificat/intuit scopul acestei [povesti](https://www.unite.ai/ro/jorge-torres-co-founder-ceo-of-mindsdb-interview-series/) de conversie si de evitare a efortului de /abstractizare si automatizare a procesar[e | ii] traditional[a | e] [ETL](https://codefinity.com/blog/What-is-the-ETL-process?utm_source=google&utm_medium=cpc&utm_campaign=21193856569&utm_content=&utm_term=&dki=&gad_source=1&gclid=CjwKCAiAyJS7BhBiEiwAyS9uNa_eEYf7AIH9uu9IcYxiQ-aiBs2SCAfM0t73VfokDo_vzZuJZKYdoBoClxcQAvD_BwE)(noul mod integrativ de a interactiona cu bazele de date dintr-o perspectiva [**AI DB - federative source data**](https://www.youtube.com/watch?v=xFTxatvBzJM)!!): ***NeStructurat***2(bidirectional)[**Structurat**](https://www.guru99.com/ro/sql-tools.html)!!!

Indiferent daca vreti sa implementati un [**RAPTOR-RAG**](https://angelina-yang.medium.com/raptor-for-advanced-rag-e0f646535c30), un [**RAPTOR**](https://www.youtube.com/watch?v=X4HPFFHjf_c), un [**RAG**](https://medium.com/the-ai-forum/implementing-advanced-rag-in-langchain-using-raptor-258a51c503c6), un **RIG** sau un simplu [**Agent**](https://github.com/mindsdb/mindsdb) puteti apela la [**MindsDB**](https://en.wikipedia.org/wiki/MindsDB) ca [integrator](https://hawatel.com/en/blog/mindsdb-what-is-it-and-what-is-it-used-for/)/[unificator](https://qdrant.tech/documentation/data-management/mindsdb/#)/[democratizator](https://hub.docker.com/r/mindsdb/mindsdb) de variate surse de date [federative](https://medium.com/israeli-tech-radar/reducing-mlops-complexity-using-sql-with-mindsdb-e20934f2e203). Pana la urma, spre exemplu, daca avem o valorificare simpla sau mixta/complexa a unui lac/masiv de date, provenind din multiple [surse de date](https://docs.mindsdb.com/integrations/data-overview),  folosind un cadru-AI-NLP(cum este LangChain, LlamaIndex...), atunci un model-**LLM**(in sens larg: **ML-engine & Model**), pe date structurate sau nestructurate, adica pe o BD relationala(ex. MySQL,PostgreSQL,...) sau vectoriala(ex. MongoDB, VectorDB,...) ..., au in cele din [urma](https://python.langchain.com/docs/integrations/providers/mindsdb/)/in [final](https://dev.to/niharikaa/mindsdb-integrate-aiml-models-into-your-applications-4oc7)/in [comun](https://github.com/aipengineer/awesome-opensource-ai-engineering/blob/main/libraries/mindsdb/README.md), ca intrare un context de interogare(***query***) si ca iesire un raspuns(***answer***), [ambele] fiind  stilizate/formulate/interfatate/automatizate in functie de contextul-utilizarii/scopului/evaluarii/valorizarii.

<hr/>


