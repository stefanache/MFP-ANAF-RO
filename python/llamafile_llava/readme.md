Server-ul Ollama uneori s-a dovedit a fi prea lent. Asa ca am decis sa creez un workshop cu serverul llamafile sub sistemul de operare Windows 10(Pro).

Despre descarcarea fisierului **llava-v1.5-7b-q4llamfile.** puteti citi [aici](https://python.langchain.com/v0.2/docs/integrations/llms/llamafile/)
