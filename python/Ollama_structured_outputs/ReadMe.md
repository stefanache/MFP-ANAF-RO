Am testat acest video intitulat [**Structured Outputs with Ollama from Local AI Models - Hands-on Tutorial**](https://www.youtube.com/watch?v=jklzyaN514w) postat de [***Fahd Mirza***](https://www.youtube.com/@fahdmirza)

[**Codul python**](https://www.fahdmirza.com/2024/12/control-llms-output-with-ollama.html) l-am gasi [aici](https://ollama.com/blog/structured-outputs)

Informatii despre [**pydantic**](https://pypi.org/project/pydantic/) puteti gasi aici.

Am folosit Ollama unde am incarcat(*pull*) cele 3 modele necesare acestui proiect

La mine a functionat foarte bine pe PC-ul meu(**HW**: i7,32MB SSD,RTX4060 si **SW**: W1indows 10 Pro, Python 3.10, Ollama 0.5.1)...

...sper sa va fie util!
